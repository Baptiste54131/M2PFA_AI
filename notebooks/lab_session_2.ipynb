{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70cbc180-f969-45e3-b6e3-81be49c2f2ff",
      "metadata": {
        "id": "70cbc180-f969-45e3-b6e3-81be49c2f2ff"
      },
      "source": [
        "Name : Lassalle Baptiste\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f876309c-c5df-4e39-98aa-800c5197e790",
      "metadata": {
        "id": "f876309c-c5df-4e39-98aa-800c5197e790"
      },
      "source": [
        "E-mail : baptiste.lassalle@live.fr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac64bcbe-63ae-4cd4-b37c-7a20e29336c1",
      "metadata": {
        "id": "ac64bcbe-63ae-4cd4-b37c-7a20e29336c1"
      },
      "source": [
        "# Useful Jupyter Notebook Shortcuts\n",
        "\n",
        "Here are some helpful keyboard shortcuts for Jupyter Notebook:\n",
        "\n",
        "- **M**: Switch to Markdown mode\n",
        "- **Y**: Switch to Code mode\n",
        "- **A**: Insert cell above\n",
        "- **B**: Insert cell below\n",
        "- **D, D**: (Press D twice) Delete selected cell\n",
        "- **Shift + Enter**: Run the current cell and move to the next\n",
        "- **Ctrl + Enter**: Run the current cell and stay on it\n",
        "- **Shift + Tab**: Show function/method documentation\n",
        "- **Ctrl + Shift + -**: Split cell at cursor\n",
        "- **Esc**: Enter command mode (blue border)\n",
        "- **Enter**: Enter edit mode (green border)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4916aef7-e133-4c5c-9a99-c7acd109af62",
      "metadata": {
        "id": "4916aef7-e133-4c5c-9a99-c7acd109af62"
      },
      "source": [
        "## default imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e019e3cf-3c57-4107-a16c-03cda08c56aa",
      "metadata": {
        "id": "e019e3cf-3c57-4107-a16c-03cda08c56aa"
      },
      "outputs": [],
      "source": [
        "# import for internal use\n",
        "from urllib.error import URLError\n",
        "from tensorflow.data import Dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory, get_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b89fc4af-d66a-4efd-bb98-2da918967a9c",
      "metadata": {
        "id": "b89fc4af-d66a-4efd-bb98-2da918967a9c"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3026b0ef-d34a-413f-8647-d01c8037c7d1",
      "metadata": {
        "id": "3026b0ef-d34a-413f-8647-d01c8037c7d1"
      },
      "outputs": [],
      "source": [
        "# data analysis import\n",
        "from skimage import io, img_as_float\n",
        "from scipy import stats\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3cb0253a-f0d7-4901-b5ca-8efa26a4882b",
      "metadata": {
        "scrolled": true,
        "id": "3cb0253a-f0d7-4901-b5ca-8efa26a4882b"
      },
      "outputs": [],
      "source": [
        "# machine learning imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Dense, Conv1D, Conv2D, UpSampling2D, MaxPooling2D, AveragePooling2D, Rescaling, Activation, Add, Flatten, Reshape\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers, regularizers\n",
        "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96eaa1a-13aa-4427-b278-64bb274364bf",
      "metadata": {
        "id": "c96eaa1a-13aa-4427-b278-64bb274364bf"
      },
      "source": [
        "## internal functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3a346daa-ab25-4bd4-afef-560de52df528",
      "metadata": {
        "id": "3a346daa-ab25-4bd4-afef-560de52df528"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "import tensorflow as tf\n",
        "\n",
        "def transpose_image_grayscale(image: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Transpose a grayscale image from portrait to landscape orientation if necessary.\n",
        "\n",
        "    This function checks the dimensions of the input image and transposes it if\n",
        "    the height (first dimension) is less than the width (second dimension).\n",
        "    The transposition ensures the image is always in landscape orientation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : tf.Tensor\n",
        "        A 3D tensor representing a grayscale image with shape (height, width, 1).\n",
        "        The last dimension represents the single color channel.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tf.Tensor\n",
        "        A 3D tensor with the same content as the input image, but guaranteed to be\n",
        "        in landscape orientation (width >= height).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function assumes the input is a valid TensorFlow tensor representing\n",
        "    a grayscale image with three dimensions (height, width, channel).\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import tensorflow as tf\n",
        "    >>> # Create a portrait grayscale image (5x3x1)\n",
        "    >>> portrait_image = tf.random.normal([5, 3, 1])\n",
        "    >>> landscape_result = transpose_image_grayscale(portrait_image)\n",
        "    >>> print(tf.shape(landscape_result).numpy())\n",
        "    [3 5 1]\n",
        "\n",
        "    >>> # Create a landscape grayscale image (3x5x1)\n",
        "    >>> landscape_image = tf.random.normal([3, 5, 1])\n",
        "    >>> result = transpose_image_grayscale(landscape_image)\n",
        "    >>> print(tf.shape(result).numpy())\n",
        "    [3 5 1]\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    tf.transpose : The underlying TensorFlow operation used for transposition.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the input tensor does not have exactly 3 dimensions.\n",
        "    \"\"\"\n",
        "    # Check if image is in portrait mode\n",
        "    if tf.shape(image)[0] < tf.shape(image)[1]:\n",
        "        # Transpose to landscape mode\n",
        "        return tf.transpose(image, perm=[1, 0, 2])  # No need for channel dim in grayscale\n",
        "    else:\n",
        "        # If it's already landscape, return the image as is\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a4e082-c320-4205-9bb8-e4b2476f4681",
      "metadata": {
        "id": "e2a4e082-c320-4205-9bb8-e4b2476f4681"
      },
      "source": [
        "## Public Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4866ea5b-fed6-492a-b49c-58f771b4a8dd",
      "metadata": {
        "id": "4866ea5b-fed6-492a-b49c-58f771b4a8dd"
      },
      "outputs": [],
      "source": [
        "def load_and_create_datasets(\n",
        "    url: str,\n",
        "    fname: str,\n",
        "    batch_size: int = 16,\n",
        "    image_size: tuple[int, int] = (480, 320)\n",
        ") -> tuple[Dataset, Dataset]:\n",
        "    \"\"\"\n",
        "    Load and prepare image datasets for denoising tasks.\n",
        "\n",
        "    This function downloads a dataset, extracts it, and creates TensorFlow datasets\n",
        "    for training and testing. It handles both original (clean) and noisy images,\n",
        "    applying necessary preprocessing steps including transposition and normalization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        URL or local path to the dataset zip file.\n",
        "    fname : str\n",
        "        Filename to save the downloaded dataset.\n",
        "    batch_size : int, optional\n",
        "        Batch size for the datasets, by default 16.\n",
        "    image_size : tuple of int, optional\n",
        "        Target size for the images (height, width), by default (480, 320).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dataset, Dataset]\n",
        "        A tuple containing:\n",
        "        - dataset_train: A TensorFlow Dataset containing pairs of (noisy, original) images for training\n",
        "        - dataset_test: A TensorFlow Dataset containing pairs of (noisy, original) images for testing\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function performs several steps:\n",
        "    1. Downloads and extracts the dataset\n",
        "    2. Creates separate datasets for original and noisy images, both for training and testing\n",
        "    3. Applies image transposition to ensure consistent orientation\n",
        "    4. Normalizes pixel values from [0, 255] to [0, 1]\n",
        "\n",
        "    The expected dataset structure after extraction is:\n",
        "    - BSD400/original/ (training original images)\n",
        "    - BSD400/noise_gaussian_15/ (training noisy images)\n",
        "    - BSD68/original/ (testing original images)\n",
        "    - BSD68/noise_gaussian_15/ (testing noisy images)\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> url = \"https://example.com/dataset.zip\"\n",
        "    >>> fname = \"denoising_dataset.zip\"\n",
        "    >>> train_ds, test_ds = load_and_create_datasets(url, fname)\n",
        "    >>> for noisy_batch, clean_batch in train_ds.take(1):\n",
        "    ...     print(f\"Batch shape: {noisy_batch.shape}\")\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    Exception\n",
        "        If the URL download fails, it attempts to use the URL as a local filename.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - All images are loaded in grayscale mode\n",
        "    - Images are automatically transposed if needed using the transpose_image_grayscale function\n",
        "    - Pixel values are rescaled from 0-255 to 0-1\n",
        "    - The function uses a specific file hash for verification\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    transpose_image_grayscale : Function used for ensuring consistent image orientation\n",
        "    tensorflow.keras.utils.get_file : Used for downloading and extracting the dataset\n",
        "    \"\"\"\n",
        "    image_size = tuple(image_size[0:2])\n",
        "    dset_name = Path(fname).stem\n",
        "\n",
        "    FileHash = r\"d68e70fa5e9ba1ae4d36dd40a0095f1e1bb49d6574c0372eb5079c636fce0651\"\n",
        "\n",
        "    try:\n",
        "        dset_download = get_file(fname=fname,\n",
        "            origin=url,\n",
        "            extract=True,\n",
        "            file_hash=FileHash,\n",
        "            archive_format=\"zip\",\n",
        "            force_download=False\n",
        "        )\n",
        "    except Exception:\n",
        "            print(\"URL download failed, try using DATASET_URL as a local filename\")\n",
        "            dset_download = get_file(fname=fname,\n",
        "                                          origin  = \"file:\\\\\"+url,\n",
        "            extract=True,\n",
        "            file_hash=FileHash,\n",
        "            archive_format=\"zip\",\n",
        "            force_download=False\n",
        "        )\n",
        "\n",
        "\n",
        "    # Step 2: Get the path of the extracted directory\n",
        "    dataset_dir = os.path.dirname(dset_download)\n",
        "    image_dataset_from_directory_common_args = {\n",
        "        \"label_mode\":None,\n",
        "        \"class_names\":None,\n",
        "        \"color_mode\":\"grayscale\",\n",
        "        \"batch_size\":batch_size,\n",
        "        \"image_size\":image_size,\n",
        "        \"shuffle\":False,\n",
        "        \"seed\":42,\n",
        "        \"validation_split\":None,\n",
        "        \"subset\":None,\n",
        "        \"interpolation\":\"bilinear\",\n",
        "        \"follow_links\":False,\n",
        "        \"crop_to_aspect_ratio\":False,\n",
        "        \"pad_to_aspect_ratio\":False,\n",
        "        \"data_format\":None,\n",
        "        \"verbose\":False,\n",
        "    }\n",
        "\n",
        "    train_images_original = image_dataset_from_directory(\n",
        "        Path(dataset_dir).joinpath(dset_name, \"BSD400\",\"original\"),\n",
        "        **image_dataset_from_directory_common_args\n",
        "    )\n",
        "\n",
        "    train_images_noisy = image_dataset_from_directory(\n",
        "        Path(dataset_dir).joinpath(dset_name, \"BSD400\",\"noise_gaussian_15\"),\n",
        "        **image_dataset_from_directory_common_args\n",
        "    )\n",
        "\n",
        "    test_images_original = image_dataset_from_directory(\n",
        "        Path(dataset_dir).joinpath(dset_name, \"BSD68\", \"original\"),\n",
        "        **image_dataset_from_directory_common_args\n",
        "    )\n",
        "\n",
        "    test_images_noisy = image_dataset_from_directory(\n",
        "        Path(dataset_dir).joinpath(dset_name, \"BSD68\", \"noise_gaussian_15\"),\n",
        "        **image_dataset_from_directory_common_args\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Apply the transpose function for grayscale images\n",
        "    train_images_original = train_images_original.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
        "    test_images_original = test_images_original.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
        "    train_images_noisy = train_images_noisy.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
        "    test_images_noisy = test_images_noisy.map(lambda x: tf.map_fn(transpose_image_grayscale, x))\n",
        "\n",
        "\n",
        "    rescale = Rescaling(1.0 / 255)\n",
        "\n",
        "    # Apply the Rescaling layer for normalization to both datasets\n",
        "    train_images_original = train_images_original.map(lambda x: rescale(x))\n",
        "    test_images_original = test_images_original.map(lambda x: rescale(x))\n",
        "    train_images_noisy = train_images_noisy.map(lambda x: rescale(x))\n",
        "    test_images_noisy = test_images_noisy.map(lambda x: rescale(x))\n",
        "\n",
        "\n",
        "    dataset_train = Dataset.zip((train_images_noisy, train_images_original))\n",
        "    dataset_test = Dataset.zip((test_images_noisy, test_images_original))\n",
        "    print(\"Datasets for train and test, created. Please note that pixels values have been rescale from 0->255 to 0->1\")\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1e499907-4247-4e24-a055-f5047ca615cb",
      "metadata": {
        "id": "1e499907-4247-4e24-a055-f5047ca615cb"
      },
      "outputs": [],
      "source": [
        "def dataset_to_list(dataset):\n",
        "    flat_images_noisy = []\n",
        "    flat_images_original = []\n",
        "\n",
        "    for noisy_batch, original_batch in dataset:\n",
        "        # Convert to numpy and flatten the batch dimension\n",
        "        noisy_images = noisy_batch.numpy()\n",
        "        original_images = original_batch.numpy()\n",
        "\n",
        "        # Extend our lists with the flattened batches\n",
        "        flat_images_noisy.extend(noisy_images)\n",
        "        flat_images_original.extend(original_images)\n",
        "\n",
        "    return tuple([x.squeeze() for x in flat_images_noisy]), tuple([x.squeeze() for x in flat_images_original])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6eee9131-9723-486d-b0a3-b45051c42fbb",
      "metadata": {
        "id": "6eee9131-9723-486d-b0a3-b45051c42fbb"
      },
      "outputs": [],
      "source": [
        "   def fourier_denoise(image, threshold=0.1):\n",
        "       # Compute the 2D FFT of the image\n",
        "       f = np.fft.fft2(image)\n",
        "\n",
        "       # Shift the zero-frequency component to the center\n",
        "       f_shift = np.fft.fftshift(f)\n",
        "\n",
        "       # Create a mask based on the threshold\n",
        "       mask = np.abs(f_shift) > threshold * np.max(np.abs(f_shift))\n",
        "\n",
        "       # Apply the mask\n",
        "       f_shift_filtered = f_shift * mask\n",
        "\n",
        "       # Shift back\n",
        "       f_filtered = np.fft.ifftshift(f_shift_filtered)\n",
        "\n",
        "       # Compute the inverse 2D FFT\n",
        "       denoised = np.real(np.fft.ifft2(f_filtered))\n",
        "\n",
        "       return np.clip(denoised, 0., 1.), f_shift, f_shift_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9825a62-9707-44b4-abcd-43c8e8786e35",
      "metadata": {
        "id": "f9825a62-9707-44b4-abcd-43c8e8786e35"
      },
      "source": [
        "## global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e1b41895-e41f-44a8-abad-f277322e0655",
      "metadata": {
        "id": "e1b41895-e41f-44a8-abad-f277322e0655"
      },
      "outputs": [],
      "source": [
        "DATASET_URL = \"https://amubox.univ-amu.fr/s/TP3mLFKikYdxt7o/download/dataset_bsd400_68.zip\"\n",
        "DATASET_FNAME = \"dataset_bsd400_68.zip\"\n",
        "# images downsampled to reduce memory usage during training\n",
        "IMAGE_SHAPE = (240, 160, 1) # last value correspond to the number of channels : 1 for grayscale, 3 for rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a33409a-1448-4b70-9caa-d096894fb2c0",
      "metadata": {
        "id": "7a33409a-1448-4b70-9caa-d096894fb2c0"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "af743f9a-ae8c-410b-a55a-1d1836eb7308",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af743f9a-ae8c-410b-a55a-1d1836eb7308",
        "outputId": "37c4c965-be0e-4b70-d2da-1d7bc034d074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://amubox.univ-amu.fr/s/TP3mLFKikYdxt7o/download/dataset_bsd400_68.zip\n",
            "\u001b[1m182766331/182766331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n",
            "Datasets for train and test, created. Please note that pixels values have been rescale from 0->255 to 0->1\n"
          ]
        }
      ],
      "source": [
        "dataset_train, dataset_test = load_and_create_datasets(DATASET_URL, DATASET_FNAME, image_size=IMAGE_SHAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1515939e-b2c3-42c5-9e0c-a7199d3922df",
      "metadata": {
        "id": "1515939e-b2c3-42c5-9e0c-a7199d3922df"
      },
      "outputs": [],
      "source": [
        "images_noise_test, images_ref_test = dataset_to_list(dataset_test)\n",
        "images_noise_train, images_ref_train = dataset_to_list(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c60b451-5280-4a78-833e-85e4d8453e7c",
      "metadata": {
        "id": "7c60b451-5280-4a78-833e-85e4d8453e7c"
      },
      "source": [
        "## Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3a7aab-e4b9-4ffd-9a16-f6f8a89346c1",
      "metadata": {
        "id": "2b3a7aab-e4b9-4ffd-9a16-f6f8a89346c1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedc6a55-c222-47e8-b3ae-455669f837f0",
      "metadata": {
        "id": "fedc6a55-c222-47e8-b3ae-455669f837f0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c2d1ae4f-3f16-4007-8be3-01aba2730fc7",
      "metadata": {
        "id": "c2d1ae4f-3f16-4007-8be3-01aba2730fc7"
      },
      "source": [
        "## Perceptron Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958390a7-97a4-4b02-9776-d532801c4f48",
      "metadata": {
        "id": "958390a7-97a4-4b02-9776-d532801c4f48"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89d4110-5b06-4716-b7a7-ef26291e1a1d",
      "metadata": {
        "id": "f89d4110-5b06-4716-b7a7-ef26291e1a1d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "06233117-2523-4711-b72b-d82af0640d4d",
      "metadata": {
        "id": "06233117-2523-4711-b72b-d82af0640d4d"
      },
      "source": [
        "## CNN Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb47a7c-d27c-47ef-aeb4-71a9be3ef223",
      "metadata": {
        "id": "1fb47a7c-d27c-47ef-aeb4-71a9be3ef223"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d7531e-b828-4a72-9cbc-429f80de028f",
      "metadata": {
        "id": "e6d7531e-b828-4a72-9cbc-429f80de028f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "37059e63-8522-4e83-b659-301d5119227f",
      "metadata": {
        "id": "37059e63-8522-4e83-b659-301d5119227f"
      },
      "source": [
        "## Conclustions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b778c7c-ed23-4477-b0e8-bfbb7ed20aa8",
      "metadata": {
        "id": "0b778c7c-ed23-4477-b0e8-bfbb7ed20aa8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef0c7c7-7f30-44b7-9986-a6691a4db807",
      "metadata": {
        "id": "0ef0c7c7-7f30-44b7-9986-a6691a4db807"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}